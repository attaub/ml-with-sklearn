#+TITLE: 
#+AUTHOR: ATTA
#+STARTUP: overview
#+OPTIONS: toc:2

* Table of contents :toc:
- [[#chapter-3][CHAPTER 3]]
  - [[#classification][Classification]]

* CHAPTER 3
** Classification
*** MNIST
MNIST datasets
70,000 small images, school kids, US Census Bureau
Each image (28x28) has a label
Images are flattende: 784 features
Create a train test sets 60000 and 10000
*** Binary Classifier
Train a binary classifier (if an images contains the digit 5)
`SGDclassifier`:
    - Can handel large datasets efficiently
    - In part, because, it deals with training instances independly (one at a time)
*** Performance Measures
Accuracy using cross validation
    Skewed dataset, accuracy is not a good measure
    
Confusion Matrix
   Conunt the number of times instance A is classified as instance B
   `crossValScore()`: returns the scores (based on some performance measure)
   `crossValPredict()`: returns the preductions

   Precision:  TP/(TP+TP), accuracy of positive prediction
        Good for video detection for kids
   Recall: (or sensitivity or TPR)  TP/(TP+FN):
        Ratio of positive instances that are correctly detected by the classifier
        Good for detecting shop lifters

   F_1 Score:  TP/(TP+0.5*(FP+FN))
        Combine precision and recall
        Simple way for comparing two classifier

Precision/Recall Trade-Off:
    For SGDClassifier:
    It computes a score based on a decision function and threshold

How to pick up threshold value:
    `SGDClassifier.decision-function' return scroe for each instance
    Then use user specified threshold
    Use `cross-val-predict` function to get scroe of all instances
    Use `precision-recall-curve` function to get prec/recall for all
    possible thresholds

Precision vs Recall
    pick up value befor the curve drops sharply 

    High precision classifier is not good if it has low recall

ROC Curve (Receiver Operating Statistics)
    TPR (Recall) vs FPR 
    FPR: ration of negative instances that are incorrectly classified as positive

    TNR: Specity: the ration of negative class that are correcly classified as negative.

       
    A perfect classifier would have a AUCROC=1, choose the classifier with larger area.

        Remark: Use PR curve whenever +ve class is rare, or you care more about the false positives than false negatives. Otherwise use ROC curve
