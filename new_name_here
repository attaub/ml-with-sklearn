



| Feature                     | Batch Gradient Descent (BGD)                   | Stochastic Gradient Descent (SGD)       | Mini-Batch Gradient Descent (MBGD)                 |
|-----------------------------+------------------------------------------------+-----------------------------------------+----------------------------------------------------|
| Update Frequency            | After entire dataset                           | After each training sample              | After each mini-batch                              |
| Computational Efficiency    | Low (expensive for large datasets)             | High (fast updates)                     | Moderate (faster than BGD, but not as fast as SGD) |
| Convergence Path            | Smooth and deterministic                       | Noisy and fluctuating                   | Balanced (less noisy than SGD)                     |
| Memory Requirements         | High (entire dataset must fit in memory)       | Low (one example at a time)             | Moderate (depends on batch size)                   |
| Suitable for Large Datasets | Not ideal (slow and memory-intensive)          | Yes (efficient for very large datasets) | Yes (works well for large datasets)                |
| Escape Local Minima         | Can get stuck in local minima                  | Yes (randomness helps escape)           | Yes (less noisy than SGD, still has randomness)    |
| Speed of Convergence        | Slow (entire dataset processed before updates) | Slower (fluctuations reduce speed)      | Faster (compared to BGD, less noisy than SGD)      |
| Online Learning             | No (needs entire dataset)                      | Yes (works well with real-time data)    | Yes (works well with real-time data)               |
